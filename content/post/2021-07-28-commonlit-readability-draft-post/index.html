---
title: 'Commonlit: Draft Post'
author: Scott Richter
date: '2021-07-28'
slug: []
categories:
  - CommonLit
  - Projects
tags:
  - Comprehensive Post
  - Exploratory Data Analysis (EDA)
  - Machine Learning
  - NLP
---



<div id="overview" class="section level1">
<h1>Overview</h1>
<div id="predicting-the-readability-of-passages-for-commonlit" class="section level2">
<h2>Predicting the Readability of Passages for Commonlit</h2>
<p>CommonLit, an education technology NPO, is searching for an improved method of classifying the reading level of works of literature for students in grades 3-12. CommonLit asserts that existing methods are either cost-prohibitive or too reductive and that there is room for improvement. By using thousands of literary excerpts and their difficulty levels as assigned by readers, I will create a model that can better predict the reading level of a given work. If the model is successful, teachers can use it to evaluate which books and poems are appropriate for their students. When students are suitably challenged, their literacy skills will improve, which is the end goal of my project.</p>
<p>The data for my project can be found at <a href="https://www.kaggle.com/c/commonlitreadabilityprize/overview">this Kaggle link.</a></p>
</div>
<div id="context" class="section level2">
<h2>Context</h2>
<p>CommonLit is a large nonprofit organization that focuses on <strong>improving youth literacy</strong>. It provides over 20 million students and teachers with free digital reading and writing lessons for grades 3-12. CommonLit maintains a vast catalogue of open source books, articles, and poems that can be included in class materials. A feature-rich literary resource, CommonLit’s collection of passages is designed to help teachers tailor their lessons at no cost.</p>
<p>The problem CommonLit currently faces is that <strong>existing methods of text readability classification are either cost-prohibitive or insufficient</strong>. The NPO would like <strong>an algorithm that rates the complexity of reading passages</strong>. With this open-source algorithm, CommonLit can help literacy curriculum developers by suggesting works of appropriate complexity. The curriculum developers would be able to <strong>quickly evaluate the complexity of literary works quickly for free</strong>. Most importantly, my project will <strong>make it easier for students to improve their reading skills</strong> through feedback the algorithm provides about the complexity and readability of their work.</p>
</div>
<div id="my-proposal" class="section level2">
<h2>My Proposal</h2>
<p>The competition sponsor poses the question “Can machine learning help align texts with students’ abilities to inspire learning?”
For practical purposes, I want to know which machine learning model and feature set can best predict the target readability level of a passage. I have a training set of 2834 excerpts from literature and their associated readability scores (An average from the judgments of multiple readers) that can help me answer these questions.</p>
<p>Because I will be predicting a numeric quantity, I’ll likely be investigating regression models. My outcome variable is the <a href="https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423">target readability score</a>. I chose this project because I intend to improve my proficiency in text mining and natural language processing. Because I only have text data to predict outcomes with, I’d expect every feature I engineer to be derivative of the text excerpts.</p>
<p>Some potential features derived from each passage could include:</p>
<ul>
<li>Sentence length</li>
<li>Prevalence of compound sentences</li>
<li>Prevalence of dialogue</li>
<li>Word repetition</li>
<li>Latent variable modeling through PCA</li>
<li>How many latent themes exist in a passage</li>
<li>Emotional content</li>
<li>Frequency of stopwords</li>
<li>Evolution of word complexity in passage</li>
<li>Difference in frequency distribution compared to a “standard” distribution for grade level</li>
</ul>
<p>While I don’t expect to find the optimal combination of model parameters and features, I will strive to reduce RMSE and produce something useful for CommonLit. I plan to benchmark my model against the existing methods that CommonLit finds insufficient. For example, the Fleisch-Kincaid Grade Level, which is based on simple text decoding and syntactic complexity. This includes measuring the average characters or syllables per word and the number of words per sentence.</p>
<p>To calculate my final RMSE, I will have to submit my final model and dataset to the project sponsor, who will then run the model on the test dataset and report back my RMSE. If possible, I will submit a model that just uses the Fleisch-Kincaid method to receive its RMSE as a benchmark to my final model’s RMSE. If that isn’t permissible, I will compare RMSE using a holdout sample I pull from the training dataset.</p>
</div>
<div id="tldr" class="section level2">
<h2>TL;DR</h2>
<p>CommonLit needs an affordable, open-source algorithm to predict the readability of text for the teachers and students it serves. I will be extracting features from text excerpts to predict readability, striving for the lowest RMSE I can achieve. Ideally, my model will outperform standard measures of reading complexity like the Fleisch-Kincaid score. I anticipate several limitations for my project, including sparse data from engineering text features, slow processing times that stem from working with a large corpus of text, and ensuring model validity for newer work. If possible, I’d like for this to be a parsimonious model that isn’t difficult to understand, even if it requires high-dimensional data. As I described in my proposal, the directions I plan to take my feature engineering just might lead to high dimensionality.</p>
<p>While I intend for my algorithm to help students improve their literacy scores, my project could have implications beyond this purpose. My method might be generalized to apply to other nonprofit and for-profit scenarios beyond education technology. Political campaigns could ensure that their messages are of a proper readability score for their target constituents, just like marketing copywriters could test whether their messaging is accessible for customers. Government offices and regulatory bodies could use my algorithm to make sure that official documentation is not too complex for its intended users.</p>
</div>
<div id="my-findings-for-now" class="section level2">
<h2>My Findings (For now)</h2>
</div>
</div>
<div id="exploring-the-data" class="section level1">
<h1>Exploring the Data</h1>
<p>A first look at EDA for CommonLit text excerpts.</p>
<div id="setup" class="section level2">
<h2>Setup</h2>
<div id="hyperparameters" class="section level3">
<h3>Hyperparameters</h3>
<pre class="r"><code># Set working directory and filenames
wd &lt;- &quot;C:/Users/srich/iCloudDrive/@ MSDS Round 3/2. Capstone Project/Project/CommonLitReadability/projectData&quot;
fileTrain &lt;- &quot;train.csv&quot;
fileTest &lt;- &quot;test.csv&quot;

# Random seed
set.seed(520)</code></pre>
</div>
<div id="libraries" class="section level3">
<h3>Libraries</h3>
<pre class="r"><code>library(tidyverse)      # General purpose data manipulation, analysis, and visualization
library(tidytext)       # Text manipulation and operations
library(tm)             # Text cleaning
library(kableExtra)     # Presentation of tables in markdown
library(tidymodels)     # User-friendly model development for predicting target readability</code></pre>
</div>
<div id="data-loading-and-preprocessing" class="section level3">
<h3>Data Loading and Preprocessing</h3>
<div id="load-data" class="section level4">
<h4>Load Data</h4>
<pre class="r"><code># Set working directory for data access
setwd(wd)

# Load in the training and test sets of the CommonLit data
train &lt;- read_csv(fileTrain)
test &lt;- read_csv(fileTest)</code></pre>
</div>
<div id="first-look" class="section level4">
<h4>First Look</h4>
<p>Test gives us 7 observations as a model for how submissions should be formatted. It contains the fields [id, url_legal, license, excerpt].</p>
<pre class="r"><code>head(test)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   id      url_legal               license   excerpt                             
##   &lt;chr&gt;   &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;                               
## 1 c0f722~ &lt;NA&gt;                    &lt;NA&gt;      &quot;My hope lay in Jack&#39;s promise that~
## 2 f0953f~ &lt;NA&gt;                    &lt;NA&gt;      &quot;Dotty continued to go to Mrs. Gray~
## 3 0df072~ &lt;NA&gt;                    &lt;NA&gt;      &quot;It was a bright and cheerful scene~
## 4 04caf4~ https://en.wikipedia.o~ CC BY-SA~ &quot;Cell division is the process by wh~
## 5 0e63f8~ https://en.wikipedia.o~ CC BY-SA~ &quot;Debugging is the process of findin~
## 6 12537f~ &lt;NA&gt;                    &lt;NA&gt;      &quot;To explain transitivity, let us lo~</code></pre>
<p>Train is the main dataset I will use for exploration and feature engineering. It contains the above fields and [target, standard_error].</p>
<pre class="r"><code>head(train) </code></pre>
<pre><code>## # A tibble: 6 x 6
##   id      url_legal license excerpt                        target standard_error
##   &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                           &lt;dbl&gt;          &lt;dbl&gt;
## 1 c12129~ &lt;NA&gt;      &lt;NA&gt;    &quot;When the young people return~ -0.340          0.464
## 2 85aa80~ &lt;NA&gt;      &lt;NA&gt;    &quot;All through dinner time, Mrs~ -0.315          0.481
## 3 b69ac6~ &lt;NA&gt;      &lt;NA&gt;    &quot;As Roger had predicted, the ~ -0.580          0.477
## 4 dd1000~ &lt;NA&gt;      &lt;NA&gt;    &quot;And outside before the palac~ -1.05           0.450
## 5 37c1b3~ &lt;NA&gt;      &lt;NA&gt;    &quot;Once upon a time there were ~  0.247          0.511
## 6 f9bf35~ &lt;NA&gt;      &lt;NA&gt;    &quot;Hal and Chester found ample ~ -0.862          0.481</code></pre>
<p>The first four columns in train are strings, so let’s take a look at the two numeric variables “target” and “standard_error”. These represent the average difficulty score that raters assigned to an excerpt, and the standard deviation of that score across raters. The lower the target rating, the easier it was for readers to read on average.</p>
<pre class="r"><code>summary(train[c(&quot;target&quot;, &quot;standard_error&quot;)])</code></pre>
<pre><code>##      target        standard_error  
##  Min.   :-3.6763   Min.   :0.0000  
##  1st Qu.:-1.6903   1st Qu.:0.4685  
##  Median :-0.9122   Median :0.4847  
##  Mean   :-0.9593   Mean   :0.4914  
##  3rd Qu.:-0.2025   3rd Qu.:0.5063  
##  Max.   : 1.7114   Max.   :0.6497</code></pre>
<p>Looks like the average difficulty of the excerpts is around -0.9593, with half of all difficulty scores falling between -1.69 and -0.20. If we can expect the standard deviation of these scores to be about 0.4914, giving us a 3<span class="math inline">\(\sigma\)</span> confidence interval of [-2.4335, 0.5149] for the difficulty level of 99.7% of the excerpts.</p>
</div>
<div id="excerpt-level-variables" class="section level4">
<h4>Excerpt-level variables</h4>
<p>Before I break the excerpts down into single words, I will create some excerpt-level predictors.</p>
<pre class="r"><code>#### Excerpt-level features ####
# Dialogue features
df_train &lt;- train %&gt;%
  mutate(n_dialogue = str_count(excerpt, &quot; \&quot;&quot;),
         n_said = str_count(tolower(excerpt), &quot;said&quot;),
         n_words = lengths(gregexpr(&quot;\\W+&quot;, excerpt)) + 1,
         avg_word_length = mean(sapply(excerpt, function(x)length(unlist(gregexpr(&quot; &quot;, x)))+1))
)

#### Dialogue analysis ####
# Jitterplot of Said vs. Dialogue
df_train %&gt;%
  ggplot(aes(x = n_said, y = n_dialogue)) +
    geom_jitter(alpha = 0.3, color = &quot;steelblue&quot;) +
    theme_minimal() +
    labs(x = &quot;Count of \&quot;Said\&quot;&quot;, y = &quot;Count of Dialogue&quot;,
         title = &quot;Dialogue in Test Excerpts&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code># Correlation of Said vs. Dialogue
cor(df_train$n_said, df_train$n_dialogue)</code></pre>
<pre><code>## [1] 0.5726842</code></pre>
</div>
<div id="create-a-word-level-dataset" class="section level4">
<h4>Create a word-level dataset</h4>
<p>The next step in working with text data is to break apart each paragraph into its component words for a more granular look. I do this using tidytext.</p>
<pre class="r"><code>#### Word-level features ####
# Create a dataset of all words, no punctuation
words &lt;- train %&gt;% 
  select(id, excerpt, target) %&gt;%
  unnest_tokens(word, excerpt)

# Convert all words to lowercase, remove punctuation, numbers, and whitespace. Drop empty words.
words_lower_nopunc &lt;- words %&gt;%
  mutate(word = tolower(word),
         word = removePunctuation(word),
         word = removeNumbers(word),
         word = gsub(&quot; &quot;, x = word, replacement = &quot;&quot;, fixed = TRUE)) %&gt;%
  filter(word != &quot;&quot;)

# View some words
head(words_lower_nopunc)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   id        target word    
##   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;   
## 1 c12129c31 -0.340 when    
## 2 c12129c31 -0.340 the     
## 3 c12129c31 -0.340 young   
## 4 c12129c31 -0.340 people  
## 5 c12129c31 -0.340 returned
## 6 c12129c31 -0.340 to</code></pre>
<pre class="r"><code># Remove stopwords
words_clean &lt;- words_lower_nopunc %&gt;%
  anti_join(stop_words)</code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code># View some clean words
head(words_clean)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   id        target word      
##   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     
## 1 c12129c31 -0.340 people    
## 2 c12129c31 -0.340 returned  
## 3 c12129c31 -0.340 ballroom  
## 4 c12129c31 -0.340 decidedly 
## 5 c12129c31 -0.340 changed   
## 6 c12129c31 -0.340 appearance</code></pre>
</div>
</div>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<div id="word-counts-and-average-difficulty" class="section level3">
<h3>Word counts and average difficulty</h3>
<pre class="r"><code># Make a dataset with word counts, mean and median difficulty scores
word_stats &lt;- words_clean %&gt;%
  group_by(word) %&gt;%
  summarize(count = n(),
            mean_difficulty = mean(target),
            median_difficulty = median(target))</code></pre>
<pre class="r"><code># 20 most common words
common &lt;- word_stats %&gt;%
  arrange(desc(count)) %&gt;%
  head(20)

ggplot(common, aes(x=reorder(word, count), y=count)) +
  geom_col(fill=&quot;darkorchid1&quot;, color = &quot;grey30&quot;) +
  coord_flip() +
  theme_light() +
  labs(x = &quot;Word&quot;, y = &quot;Count in Excerpts&quot;,
       title = &quot;20 Most Common Words in Test Data&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># 20 most difficult words appearing at least ten times
difficult &lt;- word_stats %&gt;%
  arrange(mean_difficulty) %&gt;%
  filter(count &gt;=10) %&gt;%
  head(20)

kable(difficult)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
word
</th>
<th style="text-align:right;">
count
</th>
<th style="text-align:right;">
mean_difficulty
</th>
<th style="text-align:right;">
median_difficulty
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
velocity
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
-2.694762
</td>
<td style="text-align:right;">
-2.864957
</td>
</tr>
<tr>
<td style="text-align:left;">
vertical
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
-2.626286
</td>
<td style="text-align:right;">
-2.742016
</td>
</tr>
<tr>
<td style="text-align:left;">
molecular
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
-2.617361
</td>
<td style="text-align:right;">
-2.515944
</td>
</tr>
<tr>
<td style="text-align:left;">
mm
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
-2.610755
</td>
<td style="text-align:right;">
-3.293821
</td>
</tr>
<tr>
<td style="text-align:left;">
boiler
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
-2.580915
</td>
<td style="text-align:right;">
-2.519765
</td>
</tr>
<tr>
<td style="text-align:left;">
electrostatic
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
-2.580552
</td>
<td style="text-align:right;">
-2.378186
</td>
</tr>
<tr>
<td style="text-align:left;">
electron
</td>
<td style="text-align:right;">
18
</td>
<td style="text-align:right;">
-2.563842
</td>
<td style="text-align:right;">
-2.286336
</td>
</tr>
<tr>
<td style="text-align:left;">
effected
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
-2.542675
</td>
<td style="text-align:right;">
-2.516179
</td>
</tr>
<tr>
<td style="text-align:left;">
zinc
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
-2.527868
</td>
<td style="text-align:right;">
-2.857673
</td>
</tr>
<tr>
<td style="text-align:left;">
rumania
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
-2.523840
</td>
<td style="text-align:right;">
-2.679316
</td>
</tr>
<tr>
<td style="text-align:left;">
cylinder
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
-2.491510
</td>
<td style="text-align:right;">
-3.071234
</td>
</tr>
<tr>
<td style="text-align:left;">
ingenious
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
-2.417863
</td>
<td style="text-align:right;">
-2.675046
</td>
</tr>
<tr>
<td style="text-align:left;">
cubic
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
-2.415567
</td>
<td style="text-align:right;">
-2.711545
</td>
</tr>
<tr>
<td style="text-align:left;">
rubles
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
-2.405903
</td>
<td style="text-align:right;">
-2.538207
</td>
</tr>
<tr>
<td style="text-align:left;">
tax
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
-2.389311
</td>
<td style="text-align:right;">
-2.538207
</td>
</tr>
<tr>
<td style="text-align:left;">
discrete
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
-2.388480
</td>
<td style="text-align:right;">
-2.301071
</td>
</tr>
<tr>
<td style="text-align:left;">
thickness
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
-2.384665
</td>
<td style="text-align:right;">
-2.366802
</td>
</tr>
<tr>
<td style="text-align:left;">
acceleration
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
-2.381128
</td>
<td style="text-align:right;">
-2.362779
</td>
</tr>
<tr>
<td style="text-align:left;">
alpha
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
-2.378034
</td>
<td style="text-align:right;">
-2.530228
</td>
</tr>
<tr>
<td style="text-align:left;">
babylonian
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
-2.367507
</td>
<td style="text-align:right;">
-2.232323
</td>
</tr>
</tbody>
</table>
<p>Many difficult words appear to have a scientific connotation. Words like velocity, electron, acceleration and zinc are associated with physics and chemistry.</p>
<pre class="r"><code># 20 least difficult words appearing at least ten times
easy &lt;- word_stats %&gt;%
  arrange(desc(mean_difficulty)) %&gt;%
  filter(count &gt;=10) %&gt;%
  head(20)

kable(easy)</code></pre>
<table>
<thead>
<tr>
<th style="text-align:left;">
word
</th>
<th style="text-align:right;">
count
</th>
<th style="text-align:right;">
mean_difficulty
</th>
<th style="text-align:right;">
median_difficulty
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
chameleon
</td>
<td style="text-align:right;">
15
</td>
<td style="text-align:right;">
1.0904399
</td>
<td style="text-align:right;">
0.9465676
</td>
</tr>
<tr>
<td style="text-align:left;">
bhujar
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.9271690
</td>
<td style="text-align:right;">
0.9271690
</td>
</tr>
<tr>
<td style="text-align:left;">
rini
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.9271690
</td>
<td style="text-align:right;">
0.9271690
</td>
</tr>
<tr>
<td style="text-align:left;">
parrot
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.8282761
</td>
<td style="text-align:right;">
1.2756840
</td>
</tr>
<tr>
<td style="text-align:left;">
mosquito
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.7430027
</td>
<td style="text-align:right;">
0.8663298
</td>
</tr>
<tr>
<td style="text-align:left;">
leopard
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
0.6651467
</td>
<td style="text-align:right;">
0.6593575
</td>
</tr>
<tr>
<td style="text-align:left;">
antelope
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:right;">
0.6593575
</td>
<td style="text-align:right;">
0.6593575
</td>
</tr>
<tr>
<td style="text-align:left;">
dinosaurs
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.6084755
</td>
<td style="text-align:right;">
0.9911140
</td>
</tr>
<tr>
<td style="text-align:left;">
tortoise
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
0.5778550
</td>
<td style="text-align:right;">
0.3130877
</td>
</tr>
<tr>
<td style="text-align:left;">
crocodiles
</td>
<td style="text-align:right;">
12
</td>
<td style="text-align:right;">
0.5489434
</td>
<td style="text-align:right;">
0.3853461
</td>
</tr>
<tr>
<td style="text-align:left;">
wasps
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.5460820
</td>
<td style="text-align:right;">
0.9225986
</td>
</tr>
<tr>
<td style="text-align:left;">
locusts
</td>
<td style="text-align:right;">
17
</td>
<td style="text-align:right;">
0.4927192
</td>
<td style="text-align:right;">
0.7291054
</td>
</tr>
<tr>
<td style="text-align:left;">
necklace
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
0.4714012
</td>
<td style="text-align:right;">
1.0621511
</td>
</tr>
<tr>
<td style="text-align:left;">
laura
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:right;">
0.4676447
</td>
<td style="text-align:right;">
0.7373835
</td>
</tr>
<tr>
<td style="text-align:left;">
ling
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.4476857
</td>
<td style="text-align:right;">
0.5968201
</td>
</tr>
<tr>
<td style="text-align:left;">
bunny
</td>
<td style="text-align:right;">
11
</td>
<td style="text-align:right;">
0.4337804
</td>
<td style="text-align:right;">
0.4304587
</td>
</tr>
<tr>
<td style="text-align:left;">
helen
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
0.4142795
</td>
<td style="text-align:right;">
0.6465494
</td>
</tr>
<tr>
<td style="text-align:left;">
trap
</td>
<td style="text-align:right;">
14
</td>
<td style="text-align:right;">
0.4039480
</td>
<td style="text-align:right;">
0.9322587
</td>
</tr>
<tr>
<td style="text-align:left;">
rabbits
</td>
<td style="text-align:right;">
22
</td>
<td style="text-align:right;">
0.3965288
</td>
<td style="text-align:right;">
0.6258783
</td>
</tr>
<tr>
<td style="text-align:left;">
pig
</td>
<td style="text-align:right;">
45
</td>
<td style="text-align:right;">
0.3863600
</td>
<td style="text-align:right;">
0.4599322
</td>
</tr>
</tbody>
</table>
<p>An interesting finding in line with one of my earlier assumptions. Animals are frequently represented in some of the easiest excerpts. It is unclear whether these animals are characters in a story or the excerpts are biological in nature.</p>
</div>
</div>
</div>
<div id="modeling-readability" class="section level1">
<h1>Modeling Readability</h1>
</div>
