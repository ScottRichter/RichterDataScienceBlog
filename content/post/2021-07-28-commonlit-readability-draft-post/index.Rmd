---
title: 'Commonlit: Draft Post'
author: Scott Richter
date: '2021-07-28'
slug: []
categories:
  - CommonLit
  - Projects
tags:
  - Comprehensive Post
  - Exploratory Data Analysis (EDA)
  - Machine Learning
  - NLP
---

# Overview
## Predicting the Readability of Passages for Commonlit

CommonLit, an education technology NPO, is searching for an improved method of classifying the reading level of works of literature for students in grades 3-12. CommonLit asserts that existing methods are either cost-prohibitive or too reductive and that there is room for improvement. By using thousands of literary excerpts and their difficulty levels as assigned by readers, I will create a model that can better predict the reading level of a given work. If the model is successful, teachers can use it to evaluate which books and poems are appropriate for their students. When students are suitably challenged, their literacy skills will improve, which is the end goal of my project.

The data for my project can be found at [this Kaggle link.](https://www.kaggle.com/c/commonlitreadabilityprize/overview)

## Context

CommonLit is a large nonprofit organization that focuses on **improving youth literacy**. It provides over 20 million students and teachers with free digital reading and writing lessons for grades 3-12. CommonLit maintains a vast catalogue of open source books, articles, and poems that can be included in class materials. A feature-rich literary resource, CommonLit's collection of passages is designed to help teachers tailor their lessons at no cost.

The problem CommonLit currently faces is that **existing methods of text readability classification are either cost-prohibitive or insufficient**. The NPO would like **an algorithm that rates the complexity of reading passages**. With this open-source algorithm, CommonLit can help literacy curriculum developers by suggesting works of appropriate complexity. The curriculum developers would be able to **quickly evaluate the complexity of literary works quickly for free**. Most importantly, my project will **make it easier for students to improve their reading skills** through feedback the algorithm provides about the complexity and readability of their work.


## My Proposal

The competition sponsor poses the question "Can machine learning help align texts with students' abilities to inspire learning?"
For practical purposes, I want to know which machine learning model and feature set can best predict the target readability level of a passage. I have a training set of 2834 excerpts from literature and their associated readability scores (An average from the judgments of multiple readers) that can help me answer these questions.

Because I will be predicting a numeric quantity, I'll likely be investigating regression models. My outcome variable is the [target readability score](https://www.kaggle.com/c/commonlitreadabilityprize/discussion/240423). I chose this project because I intend to improve my proficiency in text mining and natural language processing. Because I only have text data to predict outcomes with, I'd expect every feature I engineer to be derivative of the text excerpts. 

Some potential features derived from each passage could include:  

* Sentence length
* Prevalence of compound sentences
* Prevalence of dialogue
* Word repetition
* Latent variable modeling through PCA
* How many latent themes exist in a passage
* Emotional content
* Frequency of stopwords
* Evolution of word complexity in passage
* Difference in frequency distribution compared to a "standard" distribution for grade level


While I don't expect to find the optimal combination of model parameters and features, I will strive to reduce RMSE and produce something useful for CommonLit. I plan to benchmark my model against the existing methods that CommonLit finds insufficient. For example, the Fleisch-Kincaid Grade Level, which is based on simple text decoding and syntactic complexity. This includes measuring the average characters or syllables per word and the number of words per sentence. 

To calculate my final RMSE, I will have to submit my final model and dataset to the project sponsor, who will then run the model on the test dataset and report back my RMSE. If possible, I will submit a model that just uses the Fleisch-Kincaid method to receive its RMSE as a benchmark to my final model's RMSE. If that isn't permissible, I will compare RMSE using a holdout sample I pull from the training dataset.

## TL;DR
CommonLit needs an affordable, open-source algorithm to predict the readability of text for the teachers and students it serves. I will be extracting features from text excerpts to predict readability, striving for the lowest RMSE I can achieve. Ideally, my model will outperform standard measures of reading complexity like the Fleisch-Kincaid score. I anticipate several limitations for my project, including sparse data from engineering text features, slow processing times that stem from working with a large corpus of text, and ensuring model validity for newer work. If possible, I'd like for this to be a parsimonious model that isn't difficult to understand, even if it requires high-dimensional data. As I described in my proposal, the directions I plan to take my feature engineering just might lead to high dimensionality.

While I intend for my algorithm to help students improve their literacy scores, my project could have implications beyond this purpose. My method might be generalized to apply to other nonprofit and for-profit scenarios beyond education technology. Political campaigns could ensure that their messages are of a proper readability score for their target constituents, just like marketing copywriters could test whether their messaging is accessible for customers. Government offices and regulatory bodies could use my algorithm to make sure that official documentation is not too complex for its intended users.

## My Findings (For now)

# Exploring the Data
A first look at EDA for CommonLit text excerpts.

## Setup
### Hyperparameters
```{r Hyperparams}
# Set working directory and filenames
wd <- "C:/Users/srich/iCloudDrive/@ MSDS Round 3/2. Capstone Project/Project/CommonLitReadability/projectData"
fileTrain <- "train.csv"
fileTest <- "test.csv"

# Random seed
set.seed(520)
```

### Libraries
```{r Libs, message=FALSE}
library(tidyverse)      # General purpose data manipulation, analysis, and visualization
library(tidytext)       # Text manipulation and operations
library(tm)             # Text cleaning
library(kableExtra)     # Presentation of tables in markdown
library(tidymodels)     # User-friendly model development for predicting target readability
```


### Data Loading and Preprocessing
#### Load Data
```{r Load, message=FALSE, warning=FALSE}
# Set working directory for data access
setwd(wd)

# Load in the training and test sets of the CommonLit data
train <- read_csv(fileTrain)
test <- read_csv(fileTest)
```

#### First Look
Test gives us 7 observations as a model for how submissions should be formatted. It contains the fields [id, url_legal, license, excerpt].

```{r}
head(test)
```


Train is the main dataset I will use for exploration and feature engineering. It contains the above fields and [target, standard_error].

```{r}
head(train) 
```

The first four columns in train are strings, so let's take a look at the two numeric variables "target" and "standard_error". These represent the average difficulty score that raters assigned to an excerpt, and the standard deviation of that score across raters. The lower the target rating, the easier it was for readers to read on average.

```{r}
summary(train[c("target", "standard_error")])
```

Looks like the average difficulty of the excerpts is around -0.9593, with half of all difficulty scores falling between -1.69 and -0.20. If we can expect the standard deviation of these scores to be about 0.4914, giving us a 3$\sigma$ confidence interval of `r paste0("[", as.character(-0.9593 - (3*0.4914)), ", ", as.character(-0.9593 + (3*0.4914)), "]")` for the difficulty level of 99.7% of the excerpts.

#### Excerpt-level variables

Before I break the excerpts down into single words, I will create some excerpt-level predictors.

```{r}
#### Excerpt-level features ####
# Dialogue features
df_train <- train %>%
  mutate(n_dialogue = str_count(excerpt, " \""),
         n_said = str_count(tolower(excerpt), "said"),
         n_words = lengths(gregexpr("\\W+", excerpt)) + 1,
         n_stopwords = (unlist(str_split(excerpt, " "))[unlist(str_split(excerpt, " ")) %in% stop_words])
)

#### Dialogue analysis ####
# Jitterplot of Said vs. Dialogue
df_train %>%
  ggplot(aes(x = n_said, y = n_dialogue)) +
    geom_jitter(alpha = 0.3, color = "steelblue") +
    theme_minimal() +
    labs(x = "Count of \"Said\"", y = "Count of Dialogue",
         title = "Dialogue in Test Excerpts")

# Correlation of Said vs. Dialogue
cor(df_train$n_said, df_train$n_dialogue)
```


#### Create a word-level dataset

The next step in working with text data is to break apart each paragraph into its component words for a more granular look. I do this using tidytext.

```{r}
#### Word-level features ####
# Create a dataset of all words, no punctuation
words <- train %>% 
  select(id, excerpt, target) %>%
  unnest_tokens(word, excerpt)

# Convert all words to lowercase, remove punctuation, numbers, and whitespace. Drop empty words.
words_lower_nopunc <- words %>%
  mutate(word = tolower(word),
         word = removePunctuation(word),
         word = removeNumbers(word),
         word = gsub(" ", x = word, replacement = "", fixed = TRUE)) %>%
  filter(word != "")

# View some words
head(words_lower_nopunc)

# Remove stopwords
words_clean <- words_lower_nopunc %>%
  anti_join(stop_words)

# View some clean words
head(words_clean)
```


## Analysis
### Word counts and average difficulty
```{r}
# Make a dataset with word counts, mean and median difficulty scores
word_stats <- words_clean %>%
  group_by(word) %>%
  summarize(count = n(),
            mean_difficulty = mean(target),
            median_difficulty = median(target))
```

```{r}
# 20 most common words
common <- word_stats %>%
  arrange(desc(count)) %>%
  head(20)

ggplot(common, aes(x=reorder(word, count), y=count)) +
  geom_col(fill="darkorchid1", color = "grey30") +
  coord_flip() +
  theme_light() +
  labs(x = "Word", y = "Count in Excerpts",
       title = "20 Most Common Words in Test Data")
```

```{r}
# 20 most difficult words appearing at least ten times
difficult <- word_stats %>%
  arrange(mean_difficulty) %>%
  filter(count >=10) %>%
  head(20)

kable(difficult)
```

Many difficult words appear to have a scientific connotation. Words like velocity, electron, acceleration and zinc are associated with physics and chemistry. 

```{r}
# 20 least difficult words appearing at least ten times
easy <- word_stats %>%
  arrange(desc(mean_difficulty)) %>%
  filter(count >=10) %>%
  head(20)

kable(easy)
```

An interesting finding in line with one of my earlier assumptions. Animals are frequently represented in some of the easiest excerpts. It is unclear whether these animals are characters in a story or the excerpts are biological in nature.




# Modeling Readability