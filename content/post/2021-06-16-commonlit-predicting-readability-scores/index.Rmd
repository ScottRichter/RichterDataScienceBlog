---
title: 'CommonLit: Predicting Readability Scores'
author: Scott Richter
date: '2021-06-16'
slug: []
categories:
  - Projects
  - CommonLit
tags:
  - Project Overview
  - NLP
  - Machine Learning
---

## Overview

CommonLit, an education technology NPO, is searching for an improved method of classifying the reading level of works of literature for students in grades 3-12. CommonLit asserts that existing methods are either cost-prohibitive or too reductive and that there is room for improvement. By using thousands of literary excerpts and their difficulty levels as assigned by readers, I will create a model that can better predict the reading level of a given work. If the model is successful, teachers can use it to evaluate which books and poems are appropriate for their students. When students are suitably challenged, their literacy skills will improve, which is the end goal of my project.

The data for my project can be found at [this Kaggle link.](https://www.kaggle.com/c/commonlitreadabilityprize/overview)

## Context
CommonLit is a large nonprofit organization that focuses on **improving youth literacy**. It provides over 20 million students and teachers with free digital reading and writing lessons for grades 3-12. CommonLit maintains a vast catalogue of open source books, articles, and poems that can be included in class materials. A feature-rich literary resource, CommonLit's collection of passages is designed to help teachers tailor their lessons at no cost.

The problem CommonLit currently faces is that **existing methods of text readability classification are either cost-prohibitive or insufficient**. The NPO would like **an algorithm that rates the complexity of reading passages**. With this open-source algorithm, CommonLit can help literacy curriculum developers by suggesting works of appropriate complexity. The curriculum developers would be able to **quickly evaluate the complexity of literary works quickly for free**. Most importantly, my project will **make it easier for students to improve their reading skills** through feedback the algorithm provides about the complexity and readability of their work.


## Proposal

The competition sponsor poses the question "Can machine learning help align texts with students' abilities to inspire learning?"
For practical purposes, I want to know which machine learning model and feature set can best predict the target readability level of a passage. I have a training set of 2834 excerpts from literature and their associated readability scores (An average from the judgments of multiple readers) that can help me answer these questions.

Because I will be predicting a numeric quantity (my outcome variable is the target readability score), I'll likely be investigating regression models. I chose this project because I intend to improve my proficiency in text mining and natural language processing. Because I only have text data to predict outcomes with, I'd expect every feature I engineer to be derivative of the text excerpts. 

Some potential features derived from each passage could include:  

* Sentence length
* Prevalence of compound sentences
* Prevalence of dialogue
* Word repetition
* Latent variable modeling through PCA
* How many latent themes exist in a passage
* Emotional content
* Frequency of stopwords

<!--- I think this is a good start; however, I will push you to get even more creative when thinking through potential features. For instance, you could look at characteristics of frequency distributions, evolution of word complexity in passage or difference in frequency distribution compared to a "standard" distribution for grade level. -->


While I don't expect to find the optimal combination of model parameters and features, I will strive to reduce RMSE and produce something useful for CommonLit. I plan to benchmark my model against the existing methods that CommonLit finds insufficient. For example, the Fleisch-Kincaid Grade Level, which is based on simple text decoding and syntactic complexity. This includes measuring the average characters or syllables per word and the number of words per sentence. 

To calculate my final RMSE, I will have to submit my final model and dataset to the project sponsor, who will then run the model on the test dataset and report back my RMSE. If possible, I will submit a model that just uses the Fleisch-Kincaid method to receive its RMSE as a benchmark to my final model's RMSE. If that isn't permissible, I will compare RMSE using a holdout sample I pull from the training dataset.

## Conclusion
To recap, CommonLit needs an affordable, open-source algorithm to predict the readability of text for the teachers and students it serves. I will be extracting features from text excerpts to predict readability, striving for the lowest RMSE I can achieve. Ideally, my model will outperform standard measures of reading complexity like the Fleisch-Kincaid score. I anticipate several limitations for my project, including sparse data from engineering text features, slow processing times that stem from working with a large corpus of text, and ensuring model validity for newer work. If possible, I'd like for this to be a parsimonious model that isn't difficult to understand, even if it requires high-dimensional data. As I described in my proposal, the directions I plan to take my feature engineering just might lead to high dimensionality.

While I intend for my algorithm to help students improve their literacy scores, my project could have implications beyond this purpose. My method might be generalized to apply to other nonprofit and for-profit scenarios beyond education technology. Political campaigns could ensure that their messages are of a proper readability score for their target constituents, just like marketing copywriters could test whether their messaging is accessible for customers. Government offices and regulatory bodies could use my algorithm to make sure that official documentation is not too complex for its intended users.
